{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéôÔ∏è Notebook 01 ‚Äî Audio Signal Processing Demo\n",
    "\n",
    "**Speech Emotion Recognition | PBL Project ‚Äî Semester 2**\n",
    "\n",
    "---\n",
    "\n",
    "## Goals\n",
    "1. Load a `.wav` audio file using `librosa`\n",
    "2. Visualise the **waveform** (time-domain)\n",
    "3. Visualise the **spectrogram** (frequency-domain)\n",
    "4. Apply preprocessing: silence trimming, normalization, pre-emphasis\n",
    "5. Compare waveforms before/after preprocessing\n",
    "\n",
    "---\n",
    "> **Note:** Make sure you've downloaded a small RAVDESS subset into `data/ravdess_subset/`  \n",
    "> See `data/README.md` for download instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path so we can import src/\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from src.audio_processing import AudioProcessor\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "print('‚úÖ Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pick a sample audio file\n",
    "\n",
    "Edit `SAMPLE_WAV` to point to any `.wav` from your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# EDIT THIS PATH to point to one of your RAVDESS .wav files\n",
    "# Example: '../data/ravdess_subset/Actor_01/03-01-03-01-01-01-01.wav'\n",
    "# -----------------------------------------------------------------------\n",
    "SAMPLE_WAV = '../data/ravdess_subset/Actor_01/03-01-03-01-01-01-01.wav'\n",
    "\n",
    "# Fallback: generate a synthetic tone if file not found (for demo)\n",
    "if not os.path.exists(SAMPLE_WAV):\n",
    "    print(f'‚ö†Ô∏è  File not found: {SAMPLE_WAV}')\n",
    "    print('   Generating a synthetic 440 Hz tone for demo purposes...')\n",
    "    sr = 22050\n",
    "    t  = np.linspace(0, 2, 2 * sr)\n",
    "    raw_signal = 0.5 * np.sin(2 * np.pi * 440 * t) + 0.1 * np.random.randn(len(t))\n",
    "    USING_SYNTHETIC = True\n",
    "else:\n",
    "    raw_signal, sr = librosa.load(SAMPLE_WAV, sr=22050)\n",
    "    USING_SYNTHETIC = False\n",
    "    print(f'‚úÖ Loaded: {SAMPLE_WAV}')\n",
    "\n",
    "print(f'   Sample rate : {sr} Hz')\n",
    "print(f'   Duration    : {len(raw_signal)/sr:.2f} s')\n",
    "print(f'   Samples     : {len(raw_signal)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Listen to the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(raw_signal, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time-domain Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "times = np.linspace(0, len(raw_signal) / sr, len(raw_signal))\n",
    "ax.plot(times, raw_signal, color='royalblue', linewidth=0.6)\n",
    "ax.set_xlabel('Time (seconds)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title('üîä Raw Audio Waveform')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Frequency-domain: Spectrogram\n",
    "\n",
    "A spectrogram shows **how frequencies change over time** using the Short-Time Fourier Transform (STFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot spectrogram\n",
    "D = librosa.stft(raw_signal)              # Complex STFT\n",
    "D_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)  # Convert to dB scale\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "img = librosa.display.specshow(D_db, sr=sr, x_axis='time', y_axis='hz',\n",
    "                                cmap='magma', ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set_title('üìä Log-Power Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mel Spectrogram\n",
    "\n",
    "The **Mel scale** maps frequencies to match human perception. It's the foundation for MFCC features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel = librosa.feature.melspectrogram(y=raw_signal, sr=sr, n_mels=128)\n",
    "mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "img = librosa.display.specshow(mel_db, sr=sr, x_axis='time', y_axis='mel',\n",
    "                                cmap='viridis', ax=ax)\n",
    "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "ax.set_title('üéõÔ∏è Mel Spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocessing Pipeline\n",
    "\n",
    "Using our `AudioProcessor` class:\n",
    "- **Trim silence** ‚Äî remove leading/trailing quiet parts\n",
    "- **Normalize** ‚Äî scale to [-1, 1]\n",
    "- **Pre-emphasis** ‚Äî boost high frequencies before MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = AudioProcessor(sample_rate=22050, duration=3.0)\n",
    "\n",
    "# Step 1: Trim silence\n",
    "trimmed = proc.remove_silence(raw_signal)\n",
    "\n",
    "# Step 2: Normalize\n",
    "normalized = proc.normalize(trimmed)\n",
    "\n",
    "# Step 3: Pre-emphasis\n",
    "preemph = proc.apply_preemphasis(normalized, coeff=0.97)\n",
    "\n",
    "# Visualise each step\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=False)\n",
    "\n",
    "for ax, sig, title, color in zip(\n",
    "    axes,\n",
    "    [raw_signal, normalized, preemph],\n",
    "    ['Raw Signal', 'After Trim + Normalize', 'After Pre-emphasis'],\n",
    "    ['gray', 'royalblue', 'tomato']\n",
    "):\n",
    "    t = np.linspace(0, len(sig)/sr, len(sig))\n",
    "    ax.plot(t, sig, color=color, linewidth=0.6)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Time (seconds)')\n",
    "plt.suptitle('Preprocessing Steps', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Original length : {len(raw_signal)/sr:.2f}s ({len(raw_signal)} samples)')\n",
    "print(f'After trimming  : {len(trimmed)/sr:.2f}s ({len(trimmed)} samples)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare: Happy vs Sad\n",
    "\n",
    "Load two different emotion clips and compare their waveforms & spectrograms side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# EDIT these paths to point to one HAPPY and one SAD file in your dataset\n",
    "# -----------------------------------------------------------------------\n",
    "HAPPY_WAV  = '../data/ravdess_subset/Actor_01/03-01-03-01-01-01-01.wav'  # emotion code 03 = happy\n",
    "SAD_WAV    = '../data/ravdess_subset/Actor_01/03-01-04-01-01-01-01.wav'  # emotion code 04 = sad\n",
    "\n",
    "clips = {'Happy': HAPPY_WAV, 'Sad': SAD_WAV}\n",
    "loaded = {}\n",
    "\n",
    "for label, path in clips.items():\n",
    "    if os.path.exists(path):\n",
    "        sig, _ = librosa.load(path, sr=22050)\n",
    "        loaded[label] = sig\n",
    "        print(f'  {label}: loaded {len(sig)/22050:.2f}s')\n",
    "    else:\n",
    "        print(f'  {label}: file not found ({path}). Skipping.')\n",
    "\n",
    "if len(loaded) == 2:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 6))\n",
    "    colors = ['seagreen', 'crimson']\n",
    "    labels = list(loaded.keys())\n",
    "    signals = list(loaded.values())\n",
    "\n",
    "    for i, (label, sig, color) in enumerate(zip(labels, signals, colors)):\n",
    "        # Waveform\n",
    "        t = np.linspace(0, len(sig)/22050, len(sig))\n",
    "        axes[i, 0].plot(t, sig, color=color, linewidth=0.6)\n",
    "        axes[i, 0].set_title(f'{label} ‚Äî Waveform')\n",
    "        axes[i, 0].set_ylabel('Amplitude')\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel = librosa.feature.melspectrogram(y=sig, sr=22050, n_mels=64)\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "        librosa.display.specshow(mel_db, sr=22050, x_axis='time', y_axis='mel',\n",
    "                                  ax=axes[i, 1], cmap='viridis')\n",
    "        axes[i, 1].set_title(f'{label} ‚Äî Mel Spectrogram')\n",
    "\n",
    "    for ax in axes[-1]:\n",
    "        ax.set_xlabel('Time (s)')\n",
    "\n",
    "    plt.suptitle('Happy vs Sad ‚Äî Waveform & Spectrogram', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('‚ö†Ô∏è  Both files needed for comparison ‚Äî update paths above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "| Concept | What we saw |\n",
    "|---------|-------------|\n",
    "| Waveform | Amplitude over time ‚Äî energy bursts visible |\n",
    "| Spectrogram | Frequency content over time (STFT-based) |\n",
    "| Mel Spectrogram | Perceptually scaled ‚Äî better for speech |\n",
    "| Pre-emphasis | Boosts higher frequencies before feature extraction |\n",
    "| Silence trimming | Removes non-speech regions to focus the model |\n",
    "\n",
    "üëâ **Next:** Notebook 02 ‚Äî Feature Extraction (MFCCs, Pitch, Energy)\n",
    "\n",
    "---\n",
    "*PBL Project | Speech Emotion Recognition | Semester 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
