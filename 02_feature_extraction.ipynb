{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341761d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ðŸŽ›ï¸ Notebook 02 â€” Feature Extraction for Speech Emotion Recognition\n",
    "\n",
    "**Speech Emotion Recognition | PBL Project â€” Semester 2**\n",
    "\n",
    "---\n",
    "\n",
    "## Goals\n",
    "1. Extract **MFCCs** (Mel-Frequency Cepstral Coefficients) from audio\n",
    "2. Extract additional features: **pitch**, **energy**, **ZCR**\n",
    "3. Visualize MFCC heatmaps\n",
    "4. Build the **full feature matrix** from a dataset subset\n",
    "5. Explore feature distributions across emotion classes\n",
    "\n",
    "---\n",
    "> This notebook assumes you've run Notebook 01 and understand waveforms/spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from src.audio_processing import AudioProcessor\n",
    "from src.feature_extraction import FeatureExtractor\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# RAVDESS emotion code â†’ name\n",
    "EMOTION_MAP = {\n",
    "    '01': 'neutral', '02': 'calm', '03': 'happy',\n",
    "    '04': 'sad',     '05': 'angry', '06': 'fearful',\n",
    "    '07': 'disgust', '08': 'surprised'\n",
    "}\n",
    "EMOTIONS_TO_USE = ['neutral', 'happy', 'sad']  # mid-term subset\n",
    "\n",
    "print('âœ… Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Are MFCCs?\n",
    "\n",
    "**MFCCs (Mel-Frequency Cepstral Coefficients)** are the most widely used features for speech and audio tasks.\n",
    "\n",
    "### How they're computed:\n",
    "```\n",
    "Audio signal\n",
    "    â†’ Pre-emphasis (boost high freq)\n",
    "    â†’ Frame into short windows (~25ms)\n",
    "    â†’ Apply Hamming window\n",
    "    â†’ FFT â†’ power spectrum\n",
    "    â†’ Map to Mel filterbanks (40 bins)\n",
    "    â†’ Log of filterbank energies\n",
    "    â†’ DCT â†’ MFCC coefficients (typically 13â€“40)\n",
    "```\n",
    "\n",
    "The result: a **matrix of shape (n_mfcc, n_frames)** that captures *how* the voice sounds over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# Load one sample file for demonstration\n",
    "# -----------------------------------------------------------------------\n",
    "SAMPLE_WAV = '../data/ravdess_subset/Actor_01/03-01-03-01-01-01-01.wav'\n",
    "\n",
    "proc = AudioProcessor(sample_rate=22050, duration=3.0)\n",
    "ext  = FeatureExtractor(n_mfcc=40)\n",
    "\n",
    "if os.path.exists(SAMPLE_WAV):\n",
    "    signal, sr = proc.full_preprocess(SAMPLE_WAV)\n",
    "    print(f'Loaded: {SAMPLE_WAV}')\n",
    "else:\n",
    "    print('âš ï¸  Sample file not found â€” generating synthetic signal for demo')\n",
    "    sr = 22050\n",
    "    t  = np.linspace(0, 2, 2 * sr)\n",
    "    signal = 0.5 * np.sin(2 * np.pi * 200 * t) * np.exp(-t)\n",
    "    signal = proc.normalize(signal)\n",
    "\n",
    "print(f'Signal duration : {len(signal)/sr:.2f}s')\n",
    "print(f'Sample rate     : {sr} Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute & Visualize MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw MFCC matrix (40 coefficients x n_frames)\n",
    "mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=40)\n",
    "\n",
    "print(f'MFCC matrix shape: {mfcc.shape}  (n_mfcc Ã— n_frames)')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "img = librosa.display.specshow(mfcc, x_axis='time', sr=sr,\n",
    "                                cmap='coolwarm', ax=ax)\n",
    "fig.colorbar(img, ax=ax)\n",
    "ax.set_ylabel('MFCC Coefficient #')\n",
    "ax.set_title('MFCC Heatmap (40 coefficients)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. From Matrix â†’ Feature Vector\n",
    "\n",
    "Classifiers need a **fixed-length 1-D vector** per audio clip.  \n",
    "We summarize each of the 40 coefficients with **mean** and **std** across time frames.\n",
    "\n",
    "```\n",
    "MFCC matrix  (40 Ã— n_frames)\n",
    "    â†’ mean per row  â†’ 40 values\n",
    "    â†’ std  per row  â†’ 40 values\n",
    "    â†’ concat        â†’ 80-dimensional vector\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_mean = np.mean(mfcc, axis=1)   # shape: (40,)\n",
    "mfcc_std  = np.std(mfcc,  axis=1)   # shape: (40,)\n",
    "mfcc_feat = np.concatenate([mfcc_mean, mfcc_std])  # shape: (80,)\n",
    "\n",
    "print(f'Mean vector shape : {mfcc_mean.shape}')\n",
    "print(f'Std  vector shape : {mfcc_std.shape}')\n",
    "print(f'Combined feature  : {mfcc_feat.shape}')\n",
    "print(f'First 5 means     : {mfcc_mean[:5].round(3)}')\n",
    "\n",
    "# Visualize mean MFCC profile\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "\n",
    "ax1.bar(range(40), mfcc_mean, color='steelblue', alpha=0.8)\n",
    "ax1.set_xlabel('MFCC Coefficient #')\n",
    "ax1.set_ylabel('Mean Value')\n",
    "ax1.set_title('Mean MFCC Profile')\n",
    "\n",
    "ax2.bar(range(40), mfcc_std, color='coral', alpha=0.8)\n",
    "ax2.set_xlabel('MFCC Coefficient #')\n",
    "ax2.set_ylabel('Std Dev')\n",
    "ax2.set_title('MFCC Standard Deviation Profile')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MFCC Delta Features\n",
    "\n",
    "**Delta** = first derivative of MFCCs (captures rate of change)  \n",
    "**Delta-Delta** = second derivative (captures acceleration)\n",
    "\n",
    "Using all three (MFCC + Î” + Î”Î”) gives a **richer, time-aware** feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta   = librosa.feature.delta(mfcc)\n",
    "delta2  = librosa.feature.delta(mfcc, order=2)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "for ax, feat, title, cmap in zip(\n",
    "    axes,\n",
    "    [mfcc, delta, delta2],\n",
    "    ['MFCC', 'MFCC Delta (1st order)', 'MFCC Delta-Delta (2nd order)'],\n",
    "    ['coolwarm', 'RdYlBu', 'PuOr']\n",
    "):\n",
    "    librosa.display.specshow(feat, x_axis='time', sr=sr, cmap=cmap, ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.suptitle('MFCC + Delta Features', fontsize=13, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Full delta feature vector\n",
    "all_feat   = np.vstack([mfcc, delta, delta2])  # (120, n_frames)\n",
    "delta_feat = np.concatenate([np.mean(all_feat, axis=1),\n",
    "                               np.std(all_feat, axis=1)])  # (240,)\n",
    "print(f'Delta feature vector size: {delta_feat.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Other Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pitch (F0) ---\n",
    "f0 = librosa.yin(signal, fmin=50, fmax=500, sr=sr)\n",
    "f0_clean = f0[f0 > 0]  # remove unvoiced (silence) frames\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 3))\n",
    "\n",
    "# Pitch over time\n",
    "t_f0 = np.linspace(0, len(signal)/sr, len(f0))\n",
    "axes[0].plot(t_f0, f0, color='purple', linewidth=0.8)\n",
    "axes[0].set_title('Pitch (F0) over Time')\n",
    "axes[0].set_xlabel('Time (s)')\n",
    "axes[0].set_ylabel('Frequency (Hz)')\n",
    "\n",
    "# RMS Energy\n",
    "rms = librosa.feature.rms(y=signal)[0]\n",
    "t_rms = np.linspace(0, len(signal)/sr, len(rms))\n",
    "axes[1].plot(t_rms, rms, color='darkorange', linewidth=0.8)\n",
    "axes[1].set_title('RMS Energy over Time')\n",
    "axes[1].set_xlabel('Time (s)')\n",
    "axes[1].set_ylabel('Energy')\n",
    "\n",
    "# Zero Crossing Rate\n",
    "zcr = librosa.feature.zero_crossing_rate(y=signal)[0]\n",
    "t_zcr = np.linspace(0, len(signal)/sr, len(zcr))\n",
    "axes[2].plot(t_zcr, zcr, color='teal', linewidth=0.8)\n",
    "axes[2].set_title('Zero Crossing Rate over Time')\n",
    "axes[2].set_xlabel('Time (s)')\n",
    "axes[2].set_ylabel('ZCR')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Pitch stats  â€” mean: {np.mean(f0_clean):.1f} Hz, std: {np.std(f0_clean):.1f} Hz')\n",
    "print(f'Energy stats â€” mean: {np.mean(rms):.4f}, std: {np.std(rms):.4f}')\n",
    "print(f'ZCR stats    â€” mean: {np.mean(zcr):.4f}, std: {np.std(zcr):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Feature Vector via FeatureExtractor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = FeatureExtractor(n_mfcc=40)\n",
    "\n",
    "features = ext.extract_all(signal, sr,\n",
    "                             use_delta=True,\n",
    "                             use_pitch=True,\n",
    "                             use_energy=True,\n",
    "                             use_zcr=True)\n",
    "\n",
    "print(f'Full feature vector shape : {features.shape}')\n",
    "print(f'\\nBreakdown:')\n",
    "print(f'  MFCC + delta + delta2 (mean+std) : 2 Ã— 3 Ã— 40 = 240')\n",
    "print(f'  Pitch (mean, std, median)         :  3')\n",
    "print(f'  Energy (mean, std)                :  2')\n",
    "print(f'  ZCR (mean, std)                   :  2')\n",
    "print(f'  Total                             : 247')\n",
    "\n",
    "# Quick sanity check\n",
    "assert features.shape[0] == 247, f'Unexpected shape: {features.shape}'\n",
    "print('\\nâœ… Feature vector shape verified: 247')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Full Feature Matrix From Dataset\n",
    "\n",
    "Extract features from every audio file in the dataset and save as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/ravdess_subset'\n",
    "EMOTIONS_TO_USE = ['neutral', 'happy', 'sad']\n",
    "\n",
    "def get_emotion_from_filename(filename):\n",
    "    \"\"\"Extract emotion label from RAVDESS filename.\"\"\"\n",
    "    parts = os.path.basename(filename).replace('.wav','').split('-')\n",
    "    if len(parts) < 7:\n",
    "        return None\n",
    "    return EMOTION_MAP.get(parts[2])\n",
    "\n",
    "# Collect all .wav files\n",
    "wav_files = []\n",
    "for root, _, files in os.walk(DATA_DIR):\n",
    "    for f in files:\n",
    "        if f.endswith('.wav'):\n",
    "            wav_files.append(os.path.join(root, f))\n",
    "\n",
    "print(f'Found {len(wav_files)} .wav files in {DATA_DIR}')\n",
    "\n",
    "if len(wav_files) == 0:\n",
    "    print('\\nâš ï¸  No files found. Simulating a small dataset for demo...')\n",
    "    # Simulate 15 samples (5 per emotion) with random features\n",
    "    np.random.seed(42)\n",
    "    n_emotions = len(EMOTIONS_TO_USE)\n",
    "    rows = []\n",
    "    for i, emo in enumerate(EMOTIONS_TO_USE):\n",
    "        for j in range(5):\n",
    "            # Simulate slightly different feature distributions per emotion\n",
    "            fake_feat = np.random.randn(247) + i * 0.5\n",
    "            row = {'label': emo}\n",
    "            for k, v in enumerate(fake_feat):\n",
    "                row[f'feat_{k}'] = round(v, 4)\n",
    "            rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f'  Simulated DataFrame: {df.shape}')\n",
    "else:\n",
    "    proc = AudioProcessor(sample_rate=22050, duration=3.0)\n",
    "    ext  = FeatureExtractor(n_mfcc=40)\n",
    "    rows = []\n",
    "    skipped = 0\n",
    "\n",
    "    for fpath in tqdm(wav_files, desc='Extracting features'):\n",
    "        emotion = get_emotion_from_filename(fpath)\n",
    "        if emotion not in EMOTIONS_TO_USE:\n",
    "            continue\n",
    "        try:\n",
    "            sig, sr_ = proc.full_preprocess(fpath)\n",
    "            feat = ext.extract_all(sig, sr_)\n",
    "            row  = {'label': emotion}\n",
    "            for k, v in enumerate(feat):\n",
    "                row[f'feat_{k}'] = round(v, 4)\n",
    "            rows.append(row)\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f'\\nDataFrame shape: {df.shape}')\n",
    "    print(f'Skipped: {skipped}')\n",
    "\n",
    "print('\\nClass distribution:')\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature matrix\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "df.to_csv('../outputs/features.csv', index=False)\n",
    "print('Features saved â†’ ../outputs/features.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Distribution by Emotion Class\n",
    "\n",
    "Visualise whether MFCC-1 and MFCC-2 values differ by emotion â€” this tells us if features are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first 2 MFCC mean features (feat_0, feat_1)\n",
    "feature_cols = [c for c in df.columns if c.startswith('feat_')]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "colors = {'neutral': 'steelblue', 'happy': 'seagreen', 'sad': 'crimson'}\n",
    "\n",
    "for ax, feat in zip(axes, ['feat_0', 'feat_1']):\n",
    "    for emo in EMOTIONS_TO_USE:\n",
    "        subset = df[df['label'] == emo][feat]\n",
    "        subset.plot.kde(ax=ax, label=emo, color=colors.get(emo, 'gray'), linewidth=2)\n",
    "    ax.set_title(f'Distribution of {feat} (MFCC coeff) by Emotion')\n",
    "    ax.set_xlabel('Feature Value')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('If distributions overlap a lot â†’ features are not very discriminative.')\n",
    "print('If they are separated â†’ the model can use them to distinguish emotions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of first 20 features\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "subset_df = df[feature_cols[:20]]\n",
    "sns.heatmap(subset_df.corr(), cmap='coolwarm', center=0, ax=ax,\n",
    "            linewidths=0.3, annot=False)\n",
    "ax.set_title('Feature Correlation Matrix (first 20 MFCC features)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pipeline Summary\n",
    "\n",
    "```\n",
    "[ .wav file ]\n",
    "    â†“  AudioProcessor.full_preprocess()\n",
    "[ signal (1-D array) ]  trim silence â†’ normalize â†’ pre-emphasis\n",
    "    â†“  FeatureExtractor.extract_all()\n",
    "[ 247-dim feature vector ]\n",
    "    â†’ MFCC (40 coeff) + Î” + Î”Î” â†’ mean+std = 240\n",
    "    â†’ Pitch (mean, std, median)  = 3\n",
    "    â†’ RMS Energy (mean, std)     = 2\n",
    "    â†’ ZCR (mean, std)            = 2\n",
    "    â†“  EmotionClassifier.train(X, y)\n",
    "[ Trained Model ]\n",
    "    â†“  clf.predict(X_test)\n",
    "[ Emotion label: 'happy' / 'sad' / 'neutral' ]\n",
    "```\n",
    "\n",
    "ðŸ‘‰ **Next:** `src/train_eval.py` â€” training & evaluation\n",
    "\n",
    "---\n",
    "*PBL Project | Speech Emotion Recognition | Semester 2*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
